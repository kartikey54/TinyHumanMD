services:
  vllm:
    image: vllm/vllm-openai:latest
    container_name: vllm-coder
    restart: unless-stopped
    ports:
      - "${VLLM_PORT:-8000}:8000"
    volumes:
      - hf-cache:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN:-}
      - VLLM_ATTENTION_BACKEND=${VLLM_ATTENTION_BACKEND:-FLASHINFER}
    ipc: host
    command: >
      --model ${MODEL:-Qwen/Qwen2.5-Coder-14B-Instruct-AWQ}
      --max-model-len ${MAX_MODEL_LEN:-32768}
      --gpu-memory-utilization ${GPU_MEM_UTIL:-0.92}
      --enforce-eager
      --host 0.0.0.0
      --port 8000
      --served-model-name coder
      --trust-remote-code
      --dtype auto
      --api-key ${VLLM_API_KEY:-sk-local}
      --enable-auto-tool-choice
      --tool-call-parser hermes
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

volumes:
  hf-cache:
