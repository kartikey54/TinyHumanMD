# ─── Model Configuration ────────────────────────────────────────────
# Best coding model for RTX 5080 (16GB VRAM):
#   Qwen2.5-Coder-14B-Instruct-AWQ  (~8GB weights, leaves room for 32K context KV cache)
#
# Alternatives (uncomment to switch):
#   MODEL=Qwen/Qwen2.5-Coder-7B-Instruct          # FP16, ~14GB, simpler but smaller
#   MODEL=Qwen/Qwen2.5-Coder-14B-Instruct-GPTQ-Int4  # GPTQ variant if AWQ has issues
#   MODEL=bartowski/Qwen2.5-Coder-32B-Instruct-AWQ    # 32B - will NOT fit in 16GB
MODEL=Qwen/Qwen2.5-Coder-14B-Instruct-AWQ

# ─── Context Length ─────────────────────────────────────────────────
# 32K is a sweet spot for the 14B AWQ on 16GB VRAM.
# Lower to 16384 if you get OOM errors. Raise to 65536 if using the 7B model.
MAX_MODEL_LEN=32768

# ─── GPU Memory ─────────────────────────────────────────────────────
# 0.92 = use 92% of VRAM. Lower if you need VRAM for other apps.
GPU_MEM_UTIL=0.92

# ─── Server ─────────────────────────────────────────────────────────
VLLM_PORT=8000
# Generate a strong key: python3 -c "import secrets; print('sk-' + secrets.token_hex(32))"
VLLM_API_KEY=sk-CHANGE_ME

# ─── Caddy Reverse Proxy (Basic Auth) ──────────────────────────────
# Used by docker-compose.proxy.yml — Caddy listens on :8001 with Basic Auth
# Username: vllm
# Generate password: python3 -c "import secrets; print(secrets.token_urlsafe(24))"
# Generate bcrypt hash: python3 -c "import bcrypt; print(bcrypt.hashpw(b'YOUR_PASSWORD', bcrypt.gensalt(rounds=14)).decode())"
CADDY_BASIC_AUTH_HASH=CHANGE_ME

# ─── Optional: HuggingFace Token ────────────────────────────────────
# Only needed for gated models. Qwen2.5-Coder is public, so not required.
# HF_TOKEN=hf_your_token_here

# ─── Advanced ───────────────────────────────────────────────────────
# FLASHINFER is fastest on Ampere+. Fall back to FLASH_ATTN if issues.
VLLM_ATTENTION_BACKEND=FLASHINFER
